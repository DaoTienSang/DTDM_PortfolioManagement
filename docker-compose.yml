version: '3.9'

services:
  web:
    build: .
    ports:
      - 8000:8000
    env_file:
      - .env
    environment:
      DB_NAME         : ${DATABASE_NAME}
      DB_USER         : ${DATABASE_USER}
      DB_PASSWORD     : ${DATABASE_PASSWORD}
      DB_HOST         : ${DATABASE_HOST}
      DB_PORT         : ${DATABASE_PORT}
    volumes:
      - ./src:/app/src
      # - ./data:/app/data
      - static_volume:/app/src/staticfiles
      - media_volume:/app/src/media
    depends_on:
      db:
        condition: service_healthy
    restart: unless-stopped
    entrypoint: ["/bin/bash"]
    command: >
      -c "cd /app/src && python manage.py migrate && python manage.py runserver 0.0.0.0:8000"

  db:
    image: postgres:15
    environment:
      POSTGRES_DB         : ${DATABASE_NAME}
      POSTGRES_USER       : ${DATABASE_USER}
      POSTGRES_PASSWORD   : ${DATABASE_PASSWORD}
    ports:
      - 5432:5432
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DATABASE_USER}"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    
  # Airflow and Kafka components
  airflow-postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-postgres-data:/var/lib/postgresql/data
    restart: always
    ports:
      - "5433:5432"  # Changed port to avoid conflict with main DB
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 3

  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.0.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    deploy:
      resources:
        limits:
          memory: 512M

  kafka-setup:
    image: confluentinc/cp-kafka:7.0.1
    container_name: kafka-setup
    depends_on:
      - kafka
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # Block until Kafka is ready
      kafka-topics --bootstrap-server kafka:9092 --list
      
      echo -e 'Creating Kafka topics...'
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic stock-topic --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic stock-history-topic --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic stock-processed-topic --replication-factor 1 --partitions 1
      
      echo -e 'Topics are ready!'
      "

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8180:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=stock-cluster
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
    depends_on:
      - kafka
      - kafka-setup
    deploy:
      resources:
        limits:
          memory: 512M

  airflow-webserver:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-webserver
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      AIRFLOW__WEBSERVER__SECRET_KEY: 9fTV3z34RmPnXZBLwPDsx8XkTDnxmHkesJ1ECmTuuQo
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      AIRFLOW__WEBSERVER__WORKERS: "2"
      AIRFLOW_HOME: "/opt/airflow"
      # Increase timeout for connections
      AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT: "120"
      AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: "60"
      # Configure hostname settings
      AIRFLOW__CORE__HOSTNAME_CALLABLE: "socket.gethostname"
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt
      - ./airflow/config/airflow.cfg:/opt/airflow/airflow.cfg
      - airflow_logs:/opt/airflow/logs
    command: >
      bash -c "pip install -r /opt/airflow/requirements.txt &&
      airflow webserver"
    user: "50000:0"
    deploy:
      resources:
        limits:
          memory: 512M
    dns:
      - 8.8.8.8
      - 8.8.4.4
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always

  airflow-scheduler:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-scheduler
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      AIRFLOW__WEBSERVER__SECRET_KEY: 9fTV3z34RmPnXZBLwPDsx8XkTDnxmHkesJ1ECmTuuQo
      AIRFLOW_HOME: "/opt/airflow"
      # Increase timeout
      AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC: "10"
      AIRFLOW__SCHEDULER__MAX_THREADS: "2"
      # Configure hosts
      AIRFLOW__CORE__HOSTNAME_CALLABLE: "socket.gethostname"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt
      - ./airflow/config/airflow.cfg:/opt/airflow/airflow.cfg
      - airflow_logs:/opt/airflow/logs
    command: >
      bash -c "pip install -r /opt/airflow/requirements.txt &&
      airflow scheduler"
    user: "50000:0"
    deploy:
      resources:
        limits:
          memory: 512M
    dns:
      - 8.8.8.8
      - 8.8.4.4
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always

  airflow-init:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-init
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      AIRFLOW__WEBSERVER__SECRET_KEY: 9fTV3z34RmPnXZBLwPDsx8XkTDnxmHkesJ1ECmTuuQo
      AIRFLOW_HOME: "/opt/airflow"
    volumes:
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt
      - ./airflow/config/airflow.cfg:/opt/airflow/airflow.cfg
      - ./airflow/dags:/opt/airflow/dags
    entrypoint: /bin/bash
    command: -c "pip install -r /opt/airflow/requirements.txt && airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com && sleep 10 && airflow dags unpause fetch_stock_to_kafka && airflow dags trigger fetch_stock_to_kafka"
    user: "50000:0"
    dns:
      - 8.8.8.8
      - 8.8.4.4
    extra_hosts:
      - "host.docker.internal:host-gateway"
      
  spark-analysis:
    image: bitnami/spark:3.5.0
    container_name: spark-stock-analysis
    depends_on:
      - kafka
      - kafka-setup
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_URL=spark://spark-analysis:7077
      - SPARK_DRIVER_MEMORY=1g
      - SPARK_EXECUTOR_MEMORY=1g
      - DATABASE_NAME=${DATABASE_NAME}
      - DATABASE_USER=${DATABASE_USER}
      - DATABASE_PASSWORD=${DATABASE_PASSWORD}
      - DATABASE_HOST=db
      - DATABASE_PORT=5432
    volumes:
      - ./spark:/opt/app
    ports:
      - "4040:4040"
    command: /bin/bash -c "cd /opt/app && pip install kafka-python numpy pandas pyarrow finplot psycopg2-binary && spark-submit --master local[*] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 /opt/app/stock_analysis.py"
    deploy:
      resources:
        limits:
          memory: 1536M
          cpus: '1.0'
    extra_hosts:
      - "host.docker.internal:host-gateway"

  kafka-postgres:
    image: python:3.9-slim
    container_name: kafka-postgres
    depends_on:
      - kafka
      - kafka-setup
      - db
    environment:
      - KAFKA_BROKER=kafka:9092
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=${DATABASE_NAME}
      - DB_USER=${DATABASE_USER}
      - DB_PASSWORD=${DATABASE_PASSWORD}
    volumes:
      - ./kafka_to_postgres.py:/app/kafka_to_postgres.py
    command: >
      bash -c "pip install kafka-python psycopg2-binary && 
      python /app/kafka_to_postgres.py"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    extra_hosts:
      - "host.docker.internal:host-gateway"
    
volumes:
  postgres_data:
  static_volume:
  media_volume:
  airflow-postgres-data:
  airflow_logs:
