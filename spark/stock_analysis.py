from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, to_json, struct, when, lit, expr, collect_list, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType, TimestampType
import pandas as pd
import numpy as np
from datetime import datetime
import time
import psycopg2
import json
import traceback

# Kh·ªüi t·∫°o SparkSession v·ªõi timeout v√† tham s·ªë ƒë·ªÉ tƒÉng t√≠nh ·ªïn ƒë·ªãnh
spark = SparkSession.builder \
    .appName("Stock Analysis") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint") \
    .config("spark.network.timeout", "800s") \
    .config("spark.executor.heartbeatInterval", "120s") \
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true") \
    .config("spark.driver.memory", "1g") \
    .config("spark.executor.memory", "1g") \
    .config("spark.memory.fraction", "0.8") \
    .config("spark.sql.shuffle.partitions", "10") \
    .getOrCreate()

# Log level
spark.sparkContext.setLogLevel("INFO")

print("Spark session created successfully")

# PostgreSQL connection parameters
pg_params = {
    'host': 'db',
    'database': 'db_for_pm',
    'user': 'airflow',
    'password': 'admin123'
}

# Ki·ªÉm tra k·∫øt n·ªëi PostgreSQL khi kh·ªüi ƒë·ªông
try:
    print(f"üîçüîçüîç ƒêANG KI·ªÇM TRA K·∫æT N·ªêI ƒê·∫æN POSTGRESQL üîçüîçüîç")
    print(f"Th√¥ng s·ªë k·∫øt n·ªëi: {pg_params}")
    test_conn = psycopg2.connect(**pg_params)
    print("‚úÖ‚úÖ‚úÖ K·∫æT N·ªêI TH·ª¨ NGHI·ªÜM T·ªöI POSTGRESQL TH√ÄNH C√îNG! ‚úÖ‚úÖ‚úÖ")
    test_cursor = test_conn.cursor()
    test_cursor.execute("SELECT version();")
    version = test_cursor.fetchone()
    print(f"PostgreSQL version: {version[0]}")
    
    # T·∫°o b·∫£ng n·∫øu ch∆∞a t·ªìn t·∫°i
    test_cursor.execute("""
    CREATE TABLE IF NOT EXISTS stock_processed_data (
        id SERIAL PRIMARY KEY,
        symbol VARCHAR(10) NOT NULL,
        current_price FLOAT,
        ceiling FLOAT,
        floor FLOAT,
        vol FLOAT,
        json_data JSONB,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)
    test_conn.commit()
    print("‚úÖ‚úÖ‚úÖ ƒê√É KI·ªÇM TRA/T·∫†O B·∫¢NG stock_processed_data TH√ÄNH C√îNG ‚úÖ‚úÖ‚úÖ")
    
    # Ki·ªÉm tra xem c√≥ d·ªØ li·ªáu trong b·∫£ng kh√¥ng
    test_cursor.execute("SELECT COUNT(*) FROM stock_processed_data")
    count = test_cursor.fetchone()[0]
    print(f"S·ªë l∆∞·ª£ng b·∫£n ghi hi·ªán c√≥ trong b·∫£ng: {count}")
    
    test_cursor.close()
    test_conn.close()
except Exception as e:
    print(f"‚ùå‚ùå‚ùå L·ªñI K·∫æT N·ªêI POSTGRESQL: {e}")
    print(f"Chi ti·∫øt l·ªói: {traceback.format_exc()}")
    print("‚ö†Ô∏è VUI L√íNG KI·ªÇM TRA L·∫†I TH√îNG S·ªê K·∫æT N·ªêI POSTGRESQL!")

# Utility function to save data to PostgreSQL
def save_to_postgres(symbol, data):
    """
    L∆∞u d·ªØ li·ªáu v√†o PostgreSQL
    - symbol: M√£ c·ªï phi·∫øu
    - data: JSON data ƒë√£ x·ª≠ l√Ω  
    """
    conn = None
    try:
        print(f"ƒêANG L∆ØU SYMBOL: {symbol} V√ÄO POSTGRESQL")
        print(f"Th√¥ng s·ªë k·∫øt n·ªëi: {pg_params}")
        conn = psycopg2.connect(**pg_params)
        cursor = conn.cursor()
        
        # T·∫°o b·∫£ng n·∫øu ch∆∞a t·ªìn t·∫°i
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS stock_processed_data (
            id SERIAL PRIMARY KEY,
            symbol VARCHAR(10) NOT NULL,
            current_price FLOAT,
            ceiling FLOAT,
            floor FLOAT,
            vol FLOAT,
            json_data JSONB,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        conn.commit()
        
        # T·∫°o l·ªõp JSONEncoder t√πy ch·ªânh ƒë·ªÉ x·ª≠ l√Ω NaN
        class CustomJSONEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, float) and np.isnan(obj):
                    return None
                return super().default(obj)
        
        # Chuy·ªÉn data th√†nh JSON string v·ªõi encoder t√πy ch·ªânh
        json_data = json.dumps(data, cls=CustomJSONEncoder)
        print(f"D·ªØ li·ªáu ƒë√£ chuy·ªÉn th√†nh JSON: {json_data[:100]}...")
        
        # Ki·ªÉm tra xem b·∫£n ghi ƒë√£ t·ªìn t·∫°i ch∆∞a
        cursor.execute("SELECT id FROM stock_processed_data WHERE symbol = %s", (symbol,))
        result = cursor.fetchone()
        
        if result:
            # C·∫≠p nh·∫≠t b·∫£n ghi
            cursor.execute("""
            UPDATE stock_processed_data 
            SET current_price = %s, ceiling = %s, floor = %s, vol = %s, 
                json_data = %s::jsonb, updated_at = CURRENT_TIMESTAMP
            WHERE symbol = %s
            """, (
                data.get('current_price_new', 0), 
                data.get('ceiling_new', 0), 
                data.get('floor_new', 0), 
                data.get('vol_new', 0),
                json_data,
                symbol
            ))
            print(f"ƒê√£ c·∫≠p nh·∫≠t d·ªØ li·ªáu c·ªßa {symbol} trong PostgreSQL th√†nh c√¥ng")
        else:
            # Th√™m b·∫£n ghi m·ªõi
            cursor.execute("""
            INSERT INTO stock_processed_data (symbol, current_price, ceiling, floor, vol, json_data)
            VALUES (%s, %s, %s, %s, %s, %s::jsonb)
            """, (
                symbol,
                data.get('current_price_new', 0), 
                data.get('ceiling_new', 0), 
                data.get('floor_new', 0), 
                data.get('vol_new', 0),
                json_data
            ))
            print(f"ƒê√£ th√™m d·ªØ li·ªáu c·ªßa {symbol} v√†o PostgreSQL th√†nh c√¥ng")
        
        conn.commit()
    except Exception as e:
        print(f"L·ªói khi l∆∞u d·ªØ li·ªáu v√†o PostgreSQL: {str(e)}")
        print(f"\nChi ti·∫øt l·ªói: {traceback.format_exc()}\n")
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()

# ƒê·ªãnh nghƒ©a schema cho d·ªØ li·ªáu JSON t·ª´ Kafka
schema = StructType([
    StructField("symbol", StringType()),
    StructField("current_price", DoubleType()),
    StructField("ceiling", DoubleType()),
    StructField("floor", DoubleType()),
    StructField("vol", DoubleType()),
    StructField("historical_data", ArrayType(
        StructType([
            StructField("time", StringType()),
            StructField("open", DoubleType()),
            StructField("high", DoubleType()),
            StructField("low", DoubleType()),
            StructField("close", DoubleType()),
            StructField("volume", DoubleType())
        ])
    ))
])

# D√πng ƒë·ªãa ch·ªâ network c·ªßa container Kafka trong Docker
kafka_bootstrap_servers = "kafka:9092"

# ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka
kafka_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
    .option("subscribe", "stock-history-topic") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000) \
    .option("failOnDataLoss", "false") \
    .load()

# Parse JSON t·ª´ Kafka
parsed_df = kafka_df \
    .selectExpr("CAST(value AS STRING) as json_data") \
    .select(from_json(col("json_data"), schema).alias("data")) \
    .select("data.*")

# Explode array ƒë·ªÉ x·ª≠ l√Ω t·ª´ng record l·ªãch s·ª≠
exploded_df = parsed_df \
    .select(
        col("symbol"),
        col("current_price"),
        col("ceiling"),
        col("floor"),
        col("vol"),
        expr("explode(historical_data) as history")
    ) \
    .select(
        col("symbol"),
        col("current_price"),
        col("ceiling"),
        col("floor"),
        col("vol"),
        col("history.time").alias("time"),
        col("history.open").alias("open"),
        col("history.high").alias("high"),
        col("history.low").alias("low"),
        col("history.close").alias("close"),
        col("history.volume").alias("volume")
    )

# Chuy·ªÉn timestamp string th√†nh timestamp th·ª±c v√† chuy·ªÉn th√†nh string date
# ƒê·ªÉ tr√°nh l·ªói datetime64 khi chuy·ªÉn sang pandas
exploded_df = exploded_df \
    .withColumn("timestamp", to_timestamp(col("time"), "yyyy-MM-dd")) \
    .withColumn("date_str", expr("date_format(timestamp, 'yyyy-MM-dd')"))

# Bi·∫øn to√†n c·ª•c ƒë·ªÉ l∆∞u tr·ªØ SparkContext state
spark_active = True

# X·ª≠ l√Ω d·ªØ li·ªáu theo batch thay v√¨ s·ª≠ d·ª•ng window functions
def process_batch(batch_df, batch_id):
    global spark_active
    
    try:
        # Ki·ªÉm tra SparkContext c√≤n ho·∫°t ƒë·ªông kh√¥ng
        if not spark_active or spark.sparkContext._jsc.sc().isStopped():
            print(f"SparkContext ƒë√£ b·ªã shutdown, kh√¥ng th·ªÉ x·ª≠ l√Ω batch {batch_id}")
            return
            
        # Ki·ªÉm tra batch c√≥ d·ªØ li·ªáu kh√¥ng
        try:
            if batch_df.isEmpty():
                print(f"Batch {batch_id} kh√¥ng c√≥ d·ªØ li·ªáu, b·ªè qua")
                return
        except Exception as e:
            print(f"Kh√¥ng th·ªÉ ki·ªÉm tra batch r·ªóng: {e}")
            # N·∫øu kh√¥ng th·ªÉ ki·ªÉm tra isEmpty, gi·∫£ ƒë·ªãnh l√† c√≥ d·ªØ li·ªáu v√† ti·∫øp t·ª•c
        
        # X√≥a c·ªôt timestamp tr∆∞·ªõc khi chuy·ªÉn sang Pandas ƒë·ªÉ tr√°nh l·ªói
        # V√† s·ª≠ d·ª•ng date_str thay th·∫ø
        batch_df_no_ts = batch_df.drop("timestamp")
        
        # L·∫•y danh s√°ch c√°c symbol ri√™ng bi·ªát trong batch n√†y ƒë·ªÉ x·ª≠ l√Ω
        symbols_in_batch = batch_df_no_ts.select("symbol").distinct().rdd.flatMap(lambda x: x).collect()
        print(f"üîç Batch {batch_id} c√≥ {len(symbols_in_batch)} m√£ c·ªï phi·∫øu: {symbols_in_batch}")
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng m√£ c·ªï phi·∫øu trong m·ªói batch ƒë·ªÉ tr√°nh qu√° t·∫£i
        if len(symbols_in_batch) > 20:
            print(f"Batch {batch_id} c√≥ qu√° nhi·ªÅu m√£ ({len(symbols_in_batch)}), ch·ªâ x·ª≠ l√Ω 20 m√£ ƒë·∫ßu ti√™n")
            symbols_in_batch = symbols_in_batch[:20]
        
        # N·∫øu batch n√†y kh√¥ng c√≥ d·ªØ li·ªáu nhi·ªÅu, ƒë·ª£i th√™m th·ªùi gian ƒë·ªÉ t√≠ch l≈©y d·ªØ li·ªáu
        if len(symbols_in_batch) < 5:  # Gi·∫£m ng∆∞·ª°ng t·ª´ 10 xu·ªëng 5
            print(f"Batch {batch_id} ch·ªâ c√≥ {len(symbols_in_batch)} m√£, ƒë·ª£i th√™m d·ªØ li·ªáu...")
            if batch_id < 3:  # Ch·ªâ ƒë·ª£i th√™m trong c√°c batch ƒë·∫ßu ti√™n
                time.sleep(30)  # Gi·∫£m th·ªùi gian ƒë·ª£i t·ª´ 60s xu·ªëng 30s
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng d·ªØ li·ªáu n·∫øu qu√° l·ªõn ƒë·ªÉ tr√°nh qu√° t·∫£i
        count = batch_df_no_ts.count()
        print(f"Batch {batch_id} c√≥ {count} records")
        
        if count > 10000:  # Gi·∫£m ng∆∞·ª°ng t·ª´ 20000 xu·ªëng 10000
            print(f"Batch {batch_id} qu√° l·ªõn ({count} records), ph√¢n chia th√†nh c√°c batch nh·ªè h∆°n")
            # Ph√¢n chia th√†nh nhi·ªÅu batch nh·ªè h∆°n
            batches = batch_df_no_ts.randomSplit([0.25, 0.25, 0.25, 0.25])  # Gi·∫£m s·ªë l∆∞·ª£ng batches
            for i, small_batch in enumerate(batches):
                try:
                    process_small_batch(small_batch, f"{batch_id}_{i}")
                except Exception as e:
                    print(f"L·ªói khi x·ª≠ l√Ω sub-batch {batch_id}_{i}: {e}")
        else:
            # X·ª≠ l√Ω batch nh·ªè
            process_small_batch(batch_df_no_ts, batch_id)
    
    except Exception as e:
        print(f"L·ªói trong process_batch {batch_id}: {e}")
        print(traceback.format_exc())

def process_small_batch(batch_df, batch_id):
    """X·ª≠ l√Ω m·ªôt batch nh·ªè d·ªØ li·ªáu"""
    
    if spark.sparkContext._jsc.sc().isStopped():
        print(f"SparkContext ƒë√£ b·ªã shutdown, kh√¥ng th·ªÉ x·ª≠ l√Ω batch {batch_id}")
        return
        
    try:
        # Chuy·ªÉn sang Pandas ƒë·ªÉ d·ªÖ t√≠nh to√°n c√°c ch·ªâ s·ªë k·ªπ thu·∫≠t
        batch_pd = batch_df.toPandas()
        
        if batch_pd.empty:
            print(f"Batch {batch_id} kh√¥ng c√≥ d·ªØ li·ªáu sau khi chuy·ªÉn sang pandas")
            return
        
        # Chuy·ªÉn date_str th√†nh datetime n·∫øu c·∫ßn
        batch_pd['date'] = pd.to_datetime(batch_pd['date_str'])
        
        # Nh√≥m theo symbol v√† s·∫Øp x·∫øp theo th·ªùi gian
        symbols = batch_pd['symbol'].unique()
        result_dfs = []
        
        for symbol in symbols:
            # L·ªçc d·ªØ li·ªáu cho m·ªói m√£ c·ªï phi·∫øu
            symbol_df = batch_pd[batch_pd['symbol'] == symbol].sort_values('date')
            
            # N·∫øu c√≥ ƒë·ªß d·ªØ li·ªáu th√¨ t√≠nh c√°c ch·ªâ s·ªë
            if len(symbol_df) > 0:
                # L∆∞u gi·ªØ gi√° tr·ªã ceiling, floor, vol
                ceiling_value = symbol_df['ceiling'].iloc[-1] if 'ceiling' in symbol_df.columns else None
                floor_value = symbol_df['floor'].iloc[-1] if 'floor' in symbol_df.columns else None
                vol_value = symbol_df['vol'].iloc[-1] if 'vol' in symbol_df.columns else None
                
                # T√≠nh MA5, MA20
                symbol_df['ma5'] = symbol_df['close'].rolling(window=5).mean()
                symbol_df['ma20'] = symbol_df['close'].rolling(window=20).mean()
                
                # T√≠nh RSI
                delta = symbol_df['close'].diff()
                gain = delta.where(delta > 0, 0).rolling(window=14).mean()
                loss = -delta.where(delta < 0, 0).rolling(window=14).mean()
                
                # X·ª≠ l√Ω tr√°nh chia cho 0
                rs = pd.Series(np.where(loss == 0, 0, gain / loss), index=loss.index)
                symbol_df['rsi'] = 100 - (100 / (1 + rs))
                
                # T√≠nh MACD
                ema12 = symbol_df['close'].ewm(span=12, adjust=False).mean()
                ema26 = symbol_df['close'].ewm(span=26, adjust=False).mean()
                
                symbol_df['macd_line'] = ema12 - ema26
                symbol_df['macd_signal'] = symbol_df['macd_line'].ewm(span=9, adjust=False).mean()
                symbol_df['macd_histogram'] = symbol_df['macd_line'] - symbol_df['macd_signal']
                
                # ƒê∆∞a ra ƒë·ªÅ xu·∫•t d·ª±a tr√™n c√°c ch·ªâ s·ªë
                conditions = [
                    # BUY: Xu h∆∞·ªõng tƒÉng (MA5 > MA20), RSI ch∆∞a qu√° mua, MACD t√≠ch c·ª±c
                    (symbol_df['ma5'] > symbol_df['ma20']) & 
                    (symbol_df['rsi'] < 70) & 
                    (symbol_df['macd_line'] > symbol_df['macd_signal']),
                    
                    # HOLD/SELL: Xu h∆∞·ªõng tƒÉng nh∆∞ng RSI qu√° mua
                    (symbol_df['ma5'] > symbol_df['ma20']) & 
                    (symbol_df['rsi'] >= 70),
                    
                    # WATCH/BUY: Xu h∆∞·ªõng gi·∫£m nh∆∞ng RSI qu√° b√°n v√† MACD t√≠ch c·ª±c
                    (symbol_df['ma5'] < symbol_df['ma20']) & 
                    (symbol_df['rsi'] <= 30) & 
                    (symbol_df['macd_line'] > symbol_df['macd_signal']),
                    
                    # SELL/AVOID: Xu h∆∞·ªõng gi·∫£m v√† MACD ti√™u c·ª±c
                    (symbol_df['ma5'] < symbol_df['ma20']) & 
                    (symbol_df['macd_line'] <= symbol_df['macd_signal'])
                ]
                
                choices = ['BUY', 'HOLD/SELL', 'WATCH/BUY', 'SELL/AVOID']
                symbol_df['suggestion'] = np.select(conditions, choices, default='HOLD')
                
                # Th√™m l√Ω do
                reasons = [
                    "Xu h∆∞·ªõng tƒÉng (MA5 > MA20). RSI ch∆∞a qu√° mua. MACD t√≠ch c·ª±c (MACD > Signal). ƒê·ªÅ xu·∫•t: MUA - Xu h∆∞·ªõng tƒÉng, RSI ch∆∞a qu√° mua, MACD t√≠ch c·ª±c.",
                    "Xu h∆∞·ªõng tƒÉng (MA5 > MA20). Qu√° mua (RSI > 70). ƒê·ªÅ xu·∫•t: C√ÇN NH·∫ÆC B√ÅN - Th·ªã tr∆∞·ªùng c√≥ d·∫•u hi·ªáu qu√° mua.",
                    "Xu h∆∞·ªõng gi·∫£m (MA5 < MA20). Qu√° b√°n (RSI < 30). MACD t√≠ch c·ª±c (MACD > Signal). ƒê·ªÅ xu·∫•t: THEO D√ïI/MUA - Th·ªã tr∆∞·ªùng ƒëang qu√° b√°n, c√≥ d·∫•u hi·ªáu ƒë·∫£o chi·ªÅu.",
                    "Xu h∆∞·ªõng gi·∫£m (MA5 < MA20). MACD ti√™u c·ª±c (MACD < Signal). ƒê·ªÅ xu·∫•t: B√ÅN/TR√ÅNH - Xu h∆∞·ªõng gi·∫£m, MACD ti√™u c·ª±c."
                ]
                symbol_df['reason'] = np.select(conditions, reasons, default="Xu h∆∞·ªõng trung t√≠nh. ƒê·ªÅ xu·∫•t: GI·ªÆ - Ch·ªù t√≠n hi·ªáu r√µ r√†ng h∆°n.")
                
                # G√°n gi√° tr·ªã ceiling, floor, vol
                symbol_df['ceiling'] = ceiling_value
                symbol_df['floor'] = floor_value
                symbol_df['vol'] = vol_value
                
                # Th√™m v√†o k·∫øt qu·∫£
                result_dfs.append(symbol_df)
        
        # K·∫øt h·ª£p t·∫•t c·∫£ k·∫øt qu·∫£
        if result_dfs:
            result_df = pd.concat(result_dfs)
            
            # Ki·ªÉm tra l·∫°i SparkContext tr∆∞·ªõc khi ti·∫øp t·ª•c
            if spark.sparkContext._jsc.sc().isStopped():
                print(f"SparkContext ƒë√£ b·ªã shutdown, kh√¥ng th·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω batch {batch_id}")
                global spark_active
                spark_active = False
                return
            
            # Chuy·ªÉn Pandas DataFrame tr·ªü l·∫°i Spark DataFrame
            processed_df = spark.createDataFrame(result_df)
            
            # Thay ƒë·ªïi c√°ch x·ª≠ l√Ω: Gom nh√≥m d·ªØ li·ªáu theo symbol
            # T·∫°o m·ªôt dataframe ri√™ng cho th√¥ng tin hi·ªán t·∫°i c·ªßa m·ªói m√£ c·ªï phi·∫øu
            current_info_df = processed_df.select(
                col("symbol"),
                col("current_price").alias("current_price_new"),
                col("ceiling").alias("ceiling_new"),
                col("floor").alias("floor_new"),
                col("vol").alias("vol_new")
            ).dropDuplicates(["symbol"])
            
            # Nh√≥m d·ªØ li·ªáu l·ªãch s·ª≠ theo m√£ c·ªï phi·∫øu
            symbols_to_process = processed_df.select("symbol").distinct().collect()
            
            for symbol_row in symbols_to_process:
                symbol = symbol_row[0]
                # L·ªçc d·ªØ li·ªáu cho symbol hi·ªán t·∫°i
                symbol_data = processed_df.filter(col("symbol") == symbol)
                current_info = current_info_df.filter(col("symbol") == symbol).first()
                
                # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu l·ªãch s·ª≠ th√†nh danh s√°ch c√°c record
                historical_records = symbol_data.select(
                    col("time"),
                    col("open"),
                    col("high"),
                    col("low"),
                    col("close"),
                    col("volume"),
                    col("ma5"),
                    col("ma20"),
                    col("rsi"),
                    col("macd_line"),
                    col("macd_signal"),
                    col("macd_histogram"),
                    col("suggestion"),
                    col("reason")
                ).collect()
                
                # T·∫°o JSON cho t·∫•t c·∫£ d·ªØ li·ªáu l·ªãch s·ª≠
                historical_data_list = []
                for record in historical_records:
                    # T·∫°o dictionary v√† x·ª≠ l√Ω NaN
                    record_dict = {
                        "time": record["time"],
                        "open": float(record["open"]),
                        "high": float(record["high"]),
                        "low": float(record["low"]),
                        "close": float(record["close"]),
                        "volume": float(record["volume"]),
                        "suggestion": record["suggestion"],
                        "reason": record["reason"]
                    }
                    
                    # X·ª≠ l√Ω c√°c tr∆∞·ªùng c√≥ th·ªÉ c√≥ gi√° tr·ªã NaN
                    for field in ["ma5", "ma20", "rsi", "macd_line", "macd_signal", "macd_histogram"]:
                        if record[field] is not None and not np.isnan(record[field]):
                            record_dict[field] = float(record[field])
                        else:
                            record_dict[field] = None
                    
                    historical_data_list.append(record_dict)
                
                # T·∫°o message JSON ho√†n ch·ªânh
                import json
                message = {
                    "symbol": symbol,
                    "current_price_new": float(current_info["current_price_new"]) if current_info["current_price_new"] is not None else 0.0,
                    "ceiling_new": float(current_info["ceiling_new"]) if current_info["ceiling_new"] is not None else 0.0,
                    "floor_new": float(current_info["floor_new"]) if current_info["floor_new"] is not None else 0.0,
                    "vol_new": float(current_info["vol_new"]) if current_info["vol_new"] is not None else 0.0,
                    "historical_processed": historical_data_list
                }
                
                # Hi·ªÉn th·ªã m·ªôt m·∫´u JSON ƒë·∫πp h∆°n trong log (gi·ªõi h·∫°n 2 b·∫£n ghi ƒë·∫ßu ti√™n)
                message_sample = message.copy()
                if len(message_sample["historical_processed"]) > 2:
                    message_sample["historical_processed"] = message_sample["historical_processed"][:2]
                    message_sample["historical_processed"].append({"note": "... c√≤n n·ªØa ..."})
                
                print(f"V√≠ d·ª• JSON cho {symbol}:")
                print(json.dumps(message_sample, indent=2, ensure_ascii=False))
                
                # L∆∞u d·ªØ li·ªáu v√†o PostgreSQL
                try:
                    save_to_postgres(symbol, message)
                    print(f"ƒê√£ l∆∞u d·ªØ li·ªáu c·ªßa {symbol} v√†o PostgreSQL th√†nh c√¥ng")
                except Exception as e:
                    print(f"L·ªói khi l∆∞u d·ªØ li·ªáu {symbol} v√†o PostgreSQL: {e}")
                
                # T·∫°o DataFrame v·ªõi m·ªôt h√†ng ch·ª©a message n√†y
                message_json = json.dumps(message, ensure_ascii=False)
                message_df = spark.createDataFrame([(symbol, message_json)], ["key", "value"])
                
                # Ghi message v√†o Kafka
                try:
                    message_df.write \
                        .format("kafka") \
                        .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
                        .option("topic", "stock-processed-topic") \
                        .save()
                    
                    print(f"ƒê√£ x·ª≠ l√Ω v√† ghi th√†nh c√¥ng d·ªØ li·ªáu cho m√£ {symbol} v·ªõi {len(historical_data_list)} b·∫£n ghi l·ªãch s·ª≠")
                except Exception as e:
                    print(f"L·ªói khi ghi d·ªØ li·ªáu v√†o Kafka cho m√£ {symbol}: {e}")
            
            # Hi·ªÉn th·ªã th√¥ng tin ƒë·ªÉ debug
            print(f"ƒê√£ x·ª≠ l√Ω t·ªïng c·ªông {len(symbols_to_process)} m√£ c·ªï phi·∫øu t·ª´ batch {batch_id}")
                
    except Exception as e:
        print(f"L·ªói trong process_small_batch {batch_id}: {e}")
        print(traceback.format_exc())

# Thi·∫øt l·∫≠p trigger ƒë·ªÉ x·ª≠ l√Ω theo t·ª´ng batch v·ªõi kho·∫£ng th·ªùi gian
query = exploded_df \
    .writeStream \
    .foreachBatch(process_batch) \
    .trigger(processingTime="1 minute") \
    .outputMode("update") \
    .start()

print(f"B·∫Øt ƒë·∫ßu ghi d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o Kafka topic: stock-processed-topic qua {kafka_bootstrap_servers}")
print("ƒêang ch·ªù d·ªØ li·ªáu...")

# Th√™m x·ª≠ l√Ω ƒë·ªÉ ƒë√≥ng Spark gracefully khi c·∫ßn
import signal

def handle_sigterm(sig, frame):
    """X·ª≠ l√Ω t√≠n hi·ªáu SIGTERM ƒë·ªÉ ƒë√≥ng Spark gracefully"""
    print("Nh·∫≠n t√≠n hi·ªáu ƒë·ªÉ d·ª´ng ·ª©ng d·ª•ng, ƒë√≥ng Spark gracefully...")
    global spark_active
    spark_active = False
    if query is not None and query.isActive:
        query.stop()
    if not spark.sparkContext._jsc.sc().isStopped():
        spark.stop()
    print("ƒê√£ d·ª´ng Spark th√†nh c√¥ng")
    
# ƒêƒÉng k√Ω x·ª≠ l√Ω t√≠n hi·ªáu
signal.signal(signal.SIGTERM, handle_sigterm)
signal.signal(signal.SIGINT, handle_sigterm)

# Ch·ªù query k·∫øt th√∫c
try:
    query.awaitTermination()
except Exception as e:
    print(f"L·ªói trong qu√° tr√¨nh streaming: {e}")
    # Th·ª≠ restart query n·∫øu c√≥ l·ªói
    if not spark.sparkContext._jsc.sc().isStopped():
        print("Th·ª≠ restart query...")
        spark_active = True
        query = exploded_df \
            .writeStream \
            .foreachBatch(process_batch) \
            .trigger(processingTime="1 minute") \
            .outputMode("update") \
            .start()
        query.awaitTermination()
finally:
    # ƒê·∫£m b·∫£o d·ª´ng gracefully khi k·∫øt th√∫c
    if not spark.sparkContext._jsc.sc().isStopped():
        spark.stop()
    print("·ª®ng d·ª•ng k·∫øt th√∫c.") 